# -*- coding: utf-8 -*-
"""Feature selection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BfwFsjgVYCfNZc_sKkf4zTffovEbKS30
"""

import os
import pandas as pd
import numpy as np

# Mount google drive to access the file
from google.colab import drive
drive.mount('/content/drive')

# Import the data file
file_path = '/content/drive/MyDrive/Colab Notebooks/diabetes_binary_5050split_health_indicators_BRFSS2015.csv'
os.chdir(os.path.dirname(file_path))

data = pd.read_csv(file_path)

# Combo column for exploratory analysis
data['Risk_Score'] = data['Smoker'].astype(int) + (1 - data['PhysActivity'].astype(int)) + (data['BMI'] > 30).astype(int)
data['HealthyDiet'] = (data['Fruits'].astype(bool)) & (data['Veggies'].astype(bool))
data['RiskScore_DietActivity'] = (1 - data['HealthyDiet'].astype(int)) + (1 - data['PhysActivity'].astype(int)) + (data['BMI'] > 30).astype(int)
data['CardioRisk'] = data['HighBP'].astype(int) + data['HighChol'].astype(int) + data['HeartDiseaseorAttack'].astype(int)
data['StrokeRisk'] = data['HighBP'].astype(int) + data['HighChol'].astype(int) + data['Stroke'].astype(int)
data['HighMentalStress'] = (data['MentHlth'].astype(int) >= 10).astype(int)
data['HighPhysicalIssues'] = (data['PhysHlth'].astype(int) >= 10).astype(int)
data['PoorGeneralHealth'] = (data['GenHlth'].astype(int) >= 4).astype(int)
data['RiskScore_HealthPerception'] = data['HighMentalStress'] + data['HighPhysicalIssues'] + data['PoorGeneralHealth']

# Define bins and labels
bins = [0, 18.4, 24.9, 29.9, 34.9, 39.9, 40]
labels = ['0–18.4', '18.5–24.9', '25–29.9', '30-34.9', '35-39.9', '40-98']

# Create a new column for the bin
data['BMI_Bin'] = pd.cut(data['BMI'], bins=bins, labels=labels, right=True, include_lowest=True)

# Define bins and labels
bins = [0, 5, 10, 15, 20, 25, 30]
labels = ['0–5', '6-10', '11-15', '16-20', '21-25', '26-30']

# Create a new column for the bin
data['PhysHlth_Bin'] = pd.cut(data['PhysHlth'], bins=bins, labels=labels, right=True, include_lowest=True)
data['MentHlth_Bin'] = pd.cut(data['MentHlth'], bins=bins, labels=labels, right=True, include_lowest=True)

"""# **Feature Selection**

**Univariate Feature Selection**
"""

from sklearn.feature_selection import SelectKBest, f_classif

X = data.drop('Diabetes_binary', axis=1)
y = data['Diabetes_binary']

# Drop non-numeric columns and rows with NaNs
X_numeric = X.drop(columns=['BMI_Bin', 'MentHlth_Bin', 'PhysHlth_Bin'])
X_numeric = X_numeric.dropna()
y_cleaned = y[X_numeric.index] # Keep corresponding y values

selector = SelectKBest(score_func=f_classif, k=10)
X_new = selector.fit_transform(X_numeric, y_cleaned)

# Show selected feature names
selected_features = X_numeric.columns[selector.get_support()]
print("Top Features:", selected_features)

"""**Recursive Feature Elimination**"""

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(solver='liblinear')
rfe = RFE(model, n_features_to_select=10)

# Use the X_numeric DataFrame which excludes non-numeric columns
fit = rfe.fit(X_numeric, y_cleaned)

selected_columns = X_numeric.columns[fit.support_]
print("Selected Features:", selected_columns)

"""**Model-Based Selection (Embedded Method)**

Use feature importances from a tree-based model like Random Forest and Xgboost
"""

from sklearn.ensemble import RandomForestClassifier

# Drop non-numeric columns before fitting the model
X_numeric = X.drop(columns=['BMI_Bin', 'MentHlth_Bin', 'PhysHlth_Bin'])

rf = RandomForestClassifier()
rf.fit(X_numeric, y)

importances = rf.feature_importances_
top_features = X_numeric.columns[np.argsort(importances)[::-1][:10]]
print("Top features:", top_features)

from xgboost import XGBClassifier

model = XGBClassifier(enable_categorical=True)
model.fit(X, y)

importances = model.feature_importances_
selected_columns = X.columns[importances > 0.01]  # Adjust threshold
print("Selected Features:", selected_columns)